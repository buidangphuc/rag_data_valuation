{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f4761b",
   "metadata": {},
   "source": [
    "# RAG Data Valuation: Real-World Experiment\n",
    "This notebook performs an experiment using the **SQuAD** dataset to verify if Data Valuation (DV) can improve RAG performance compared to a baseline retrieval approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb112fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "from src.dv.models.entities import Chunk\n",
    "from src.dv.algorithms.loo import LOOValuator\n",
    "from src.dv.evaluation.judges import MNLIJudge\n",
    "from src.dv.core import ValuationSuite\n",
    "from src.dv.evaluation.filtering import filter_negative_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befe152",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "We'll use a subset of SQuAD and the MNLI Judge for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e71f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\n",
    "    \"dataset\": \"squad\",\n",
    "    \"num_samples\": 5, # Small number for demonstration\n",
    "    \"dv_methods\": [\"LOO\"],\n",
    "    \"judge_type\": \"mnli\",\n",
    "}\n",
    "\n",
    "print(f\"Running real-world experiment with config: {config}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f699879",
   "metadata": {},
   "source": [
    "## 2. Data Loading & RAG Simulation\n",
    "We load SQuAD and simulate a RAG process where we retrieve relevant and irrelevant chunks for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdbaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_squad_samples(n=5):\n",
    "    dataset = load_dataset(\"squad\", split=\"validation\", streaming=True)\n",
    "    samples = []\n",
    "    for item in dataset.take(n):\n",
    "        # In a real RAG, these would come from a vector DB\n",
    "        # Here we use the ground truth context + some noise/distractors\n",
    "        query = item[\"question\"]\n",
    "        answer = item[\"answers\"][\"text\"][0]\n",
    "        context = item[\"context\"]\n",
    "        \n",
    "        # Split context into sentences as chunks\n",
    "        raw_chunks = context.split(\". \")\n",
    "        chunks = [Chunk(id=f\"c{i}\", text=c) for i, c in enumerate(raw_chunks[:5])]\n",
    "        \n",
    "        # Add a distractor chunk\n",
    "        chunks.append(Chunk(id=\"distractor\", text=\"The moon is made of green cheese and cats like milk.\"))\n",
    "        \n",
    "        samples.append({\n",
    "            \"query\": query,\n",
    "            \"answer\": answer,\n",
    "            \"chunks\": chunks\n",
    "        })\n",
    "    return samples\n",
    "\n",
    "data = load_squad_samples(config[\"num_samples\"])\n",
    "print(f\"Loaded {len(data)} samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d566141",
   "metadata": {},
   "source": [
    "## 3. Experiment: Baseline vs DV-Filtered\n",
    "**Baseline**: All retrieved chunks used for context.\n",
    "**DV-Filtered**: Chunks with negative marginal contribution (LOO) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Judge and Valuators\n",
    "judge = MNLIJudge()\n",
    "valuators = {\"LOO\": LOOValuator(judge)}\n",
    "suite = ValuationSuite(valuators)\n",
    "\n",
    "results = []\n",
    "\n",
    "for item in tqdm(data):\n",
    "    query = item[\"query\"]\n",
    "    answer = item[\"answer\"]\n",
    "    chunks = item[\"chunks\"]\n",
    "    \n",
    "    # Baseline Context\n",
    "    baseline_context = \" \".join([c.text for c in chunks])\n",
    "    baseline_faithfulness = judge.get_faithfulness(query, baseline_context, answer)\n",
    "    \n",
    "    # Run Data Valuation\n",
    "    dv_results = suite.evaluate_all(query, chunks, answer)\n",
    "    \n",
    "    # DV-Filtered Context\n",
    "    filtered_chunks = filter_negative_chunks(chunks, dv_results)\n",
    "    filtered_context = \" \".join([c.text for c in filtered_chunks])\n",
    "    filtered_faithfulness = judge.get_faithfulness(query, filtered_context, answer)\n",
    "    \n",
    "    results.append({\n",
    "        \"query\": query,\n",
    "        \"baseline_faith\": baseline_faithfulness,\n",
    "        \"filtered_faith\": filtered_faithfulness,\n",
    "        \"improvement\": filtered_faithfulness - baseline_faithfulness,\n",
    "        \"num_removed\": len(chunks) - len(filtered_chunks)\n",
    "    })\n",
    "\n",
    "exp_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffdd95a",
   "metadata": {},
   "source": [
    "## 4. Results Analysis\n",
    "Comparing the average faithfulness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_baseline = exp_df[\"baseline_faith\"].mean()\n",
    "avg_filtered = exp_df[\"filtered_faith\"].mean()\n",
    "\n",
    "print(f\"Average Baseline Faithfulness: {avg_baseline:.4f}\")\n",
    "print(f\"Average Filtered Faithfulness: {avg_filtered:.4f}\")\n",
    "print(f\"Average Improvement: {(avg_filtered - avg_baseline):.4f}\")\n",
    "\n",
    "# Plotting\n",
    "exp_df[[\"baseline_faith\", \"filtered_faith\"]].plot(kind=\"bar\", figsize=(12, 6))\n",
    "plt.title(\"Faithfulness Comparison: Baseline vs DV-Filtered\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Faithfulness Score\")\n",
    "plt.legend([\"Baseline\", \"DV-Filtered\"])\n",
    "plt.show()\n",
    "\n",
    "# Histogram of improvement\n",
    "exp_df[\"improvement\"].hist(bins=10)\n",
    "plt.title(\"Distribution of Faithfulness Improvement\")\n",
    "plt.xlabel(\"Improvement\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
