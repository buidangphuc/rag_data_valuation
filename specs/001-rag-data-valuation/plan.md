# Implementation Plan: RAG Data Valuation System

**Branch**: `001-rag-data-valuation` | **Date**: 2025-12-25 | **Spec**: [spec.md](file:///Users/phuc.buidang/Documents/UIT/rag_data_valuation/specs/001-rag-data-valuation/spec.md)
**Input**: Feature specification from `/specs/[###-feature-name]/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

The objective is to implement a RAG Data Valuation system that calculates the value of retrieved chunks using LOO, Shapley, and Attention-based methods. Instead of custom evaluation logic, the system will leverage the **Ragas** framework for measuring faithfulness and other RAG metrics, ensuring a standard and robust evaluation baseline.

## Technical Context

**Language/Version**: Python 3.12  
**Primary Dependencies**: `transformers`, `torch`, `openai`, `pandas`, `numpy`, `scikit-learn`, `pytest`, `jupyter`, `ragas`  
**Storage**: Local JSON/CSV files (stored in `data/results/`)  
**Testing**: `pytest` for modules, Jupyter notebooks for experimentation  
**Target Platform**: Mac (M4/MPS support) / Linux  
**Project Type**: Standalone Valuation Wrapper  
**Performance Goals**: Shapley Value calculation for $k=5$ chunks in < 30s.  
**Constraints**: Faithful Ground Truth on 0.0-1.0 continuous scale.  
**Scale/Scope**: Research framework for evaluating RAG data quality.

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

- [x] **Pythonic Excellence**: Is the codebase Python 3.12 and Ruff-compliant?
- [x] **Verification Ready**: Are `pytest` and Jupyter notebooks planned for logic and experiment validation?
- [x] **UX Consistency**: Do internal and external APIs follow predictable patterns?
- [x] **Performance Profile**: Have local compute (Torch/Transformers) and cloud scalability (OpenAI) been considered?
- [x] **Iterative Refinement**: Does the plan focus on identifying high-value data chunks for RAG improvement?
- [x] **Module Extensibility**: Do new algorithms implement the `Valuator` interface for consistent integration?
- [x] **Benchmarking Discipline**: Are experimental results validated in `dv_real_benchmark.ipynb` or equivalent notebooks?


## Project Structure

### Documentation (this feature)

```text
specs/001-rag-data-valuation/
├── plan.md              # This file
├── research.md          # Ragas integration research
├── data-model.md        # Entities for valuation
├── quickstart.md        # How to use Ragas-based evaluation
├── contracts/
│   └── valuation_interface.md # API contracts
└── tasks.md             # To be generated by /speckit.tasks
```

### Source Code

```text
src/
├── dv/
│   ├── algorithms/
│   │   ├── loo.py        # LOO Valuator
│   │   ├── shapley.py    # Shapley Valuator
│   │   └── attention.py  # Attention-based (extraction)
│   ├── evaluation/
│   │   ├── judges.py     # RagasJudge implementation
│   │   └── metrics.py    # Suite comparison metrics
│   ├── interfaces.py     # Base classes
│   └── models/
│       └── entities.py   # Data classes
└── utils/
    └── torch_utils.py    # MPS/CUDA support
```

**Structure Decision**: Standard Python package structure using `src/` layout.

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| [e.g., 4th project] | [current need] | [why 3 projects insufficient] |
| [e.g., Repository pattern] | [specific problem] | [why direct DB access insufficient] |
