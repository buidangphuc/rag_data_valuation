{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal RAG with Leave-One-Out (LOO) Testing\n",
    "\n",
    "**Mục tiêu**: Đánh giá giá trị của từng chunk dữ liệu trong hệ thống RAG bằng phương pháp Leave-One-Out.\n",
    "\n",
    "**Phương pháp**: \n",
    "- Xây dựng RAG đơn giản với vector similarity search\n",
    "- Đánh giá hiệu suất khi loại bỏ từng chunk\n",
    "- Tính LOO score để xác định chunk nào quan trọng nhất\n",
    "\n",
    "**Môi trường**: Google Colab (hoặc local với GPU/CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if running on Colab)\n",
    "# !pip install -q transformers torch sentence-transformers scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data Creation\n",
    "\n",
    "Tạo một knowledge base nhỏ về Machine Learning với 8 chunks và 5 câu hỏi test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base: 8 chunks về Machine Learning\n",
    "knowledge_chunks = [\n",
    "    \"Machine learning is a subset of artificial intelligence that enables systems to learn from data without explicit programming. It uses algorithms to identify patterns and make predictions.\",\n",
    "    \n",
    "    \"Supervised learning requires labeled training data where each example has an input and corresponding output. Common algorithms include linear regression, decision trees, and neural networks.\",\n",
    "    \n",
    "    \"Unsupervised learning works with unlabeled data to discover hidden patterns. Clustering algorithms like K-means and dimensionality reduction techniques like PCA are popular examples.\",\n",
    "    \n",
    "    \"Deep learning uses artificial neural networks with multiple layers to learn hierarchical representations. It excels at tasks like image recognition, natural language processing, and speech recognition.\",\n",
    "    \n",
    "    \"Overfitting occurs when a model learns the training data too well, including noise and outliers. This results in poor generalization to new, unseen data. Regularization techniques help prevent overfitting.\",\n",
    "    \n",
    "    \"Cross-validation is a technique to assess model performance by splitting data into training and validation sets multiple times. K-fold cross-validation is the most common approach.\",\n",
    "    \n",
    "    \"Feature engineering involves creating new features from raw data to improve model performance. It requires domain knowledge and understanding of the problem being solved.\",\n",
    "    \n",
    "    \"Gradient descent is an optimization algorithm used to minimize the loss function by iteratively adjusting model parameters. Learning rate controls the step size in each iteration.\"\n",
    "]\n",
    "\n",
    "# Test questions với ground truth answers\n",
    "test_questions = [\n",
    "    {\n",
    "        \"question\": \"What is machine learning?\",\n",
    "        \"ground_truth\": \"Machine learning is a subset of AI that enables systems to learn from data without explicit programming.\",\n",
    "        \"relevant_chunks\": [0]  # Chunk 0 chứa câu trả lời chính\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between supervised and unsupervised learning?\",\n",
    "        \"ground_truth\": \"Supervised learning uses labeled data while unsupervised learning works with unlabeled data to discover patterns.\",\n",
    "        \"relevant_chunks\": [1, 2]  # Chunks 1 và 2\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does cross-validation work?\",\n",
    "        \"ground_truth\": \"Cross-validation splits data into training and validation sets multiple times to assess model performance.\",\n",
    "        \"relevant_chunks\": [5]  # Chunk 5\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is overfitting and how to prevent it?\",\n",
    "        \"ground_truth\": \"Overfitting is when a model learns training data too well including noise. Regularization helps prevent it.\",\n",
    "        \"relevant_chunks\": [4]  # Chunk 4\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is gradient descent?\",\n",
    "        \"ground_truth\": \"Gradient descent is an optimization algorithm that minimizes loss by iteratively adjusting parameters.\",\n",
    "        \"relevant_chunks\": [7]  # Chunk 7\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base: {len(knowledge_chunks)} chunks\")\n",
    "print(f\"Test questions: {len(test_questions)} questions\")\n",
    "print(\"\\nSample chunk:\")\n",
    "print(f\"  {knowledge_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"Simple RAG system with vector similarity search\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model, chunks: List[str]):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chunks = chunks\n",
    "        self.chunk_embeddings = None\n",
    "        self._build_index()\n",
    "    \n",
    "    def _build_index(self):\n",
    "        \"\"\"Embed all chunks and store in memory\"\"\"\n",
    "        print(f\"Embedding {len(self.chunks)} chunks...\")\n",
    "        self.chunk_embeddings = self.embedding_model.encode(\n",
    "            self.chunks, \n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        print(f\"Index built: {self.chunk_embeddings.shape}\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 2) -> List[Tuple[int, str, float]]:\n",
    "        \"\"\"Retrieve top-k most similar chunks\"\"\"\n",
    "        # Embed query\n",
    "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = cosine_similarity(query_embedding, self.chunk_embeddings)[0]\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # Return (index, chunk, score)\n",
    "        results = [\n",
    "            (idx, self.chunks[idx], similarities[idx]) \n",
    "            for idx in top_indices\n",
    "        ]\n",
    "        return results\n",
    "    \n",
    "    def generate_answer(self, query: str, top_k: int = 2) -> Dict:\n",
    "        \"\"\"Generate answer by concatenating retrieved chunks\"\"\"\n",
    "        retrieved = self.retrieve(query, top_k)\n",
    "        \n",
    "        # Simple answer: concatenate retrieved chunks\n",
    "        answer = \" \".join([chunk for _, chunk, _ in retrieved])\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"retrieved_chunks\": retrieved\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RAG system\n",
    "rag = SimpleRAG(embedding_model, knowledge_chunks)\n",
    "\n",
    "# Test với một câu hỏi\n",
    "test_query = test_questions[0][\"question\"]\n",
    "result = rag.generate_answer(test_query, top_k=2)\n",
    "\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"\\nRetrieved chunks:\")\n",
    "for idx, chunk, score in result[\"retrieved_chunks\"]:\n",
    "    print(f\"  [{idx}] (score: {score:.3f}): {chunk[:100]}...\")\n",
    "print(f\"\\nGenerated answer:\\n  {result['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_retrieval_precision(retrieved_indices: List[int], relevant_indices: List[int]) -> float:\n",
    "    \"\"\"Calculate precision: how many retrieved chunks are relevant\"\"\"\n",
    "    if len(retrieved_indices) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    relevant_set = set(relevant_indices)\n",
    "    retrieved_set = set(retrieved_indices)\n",
    "    \n",
    "    correct = len(relevant_set.intersection(retrieved_set))\n",
    "    return correct / len(retrieved_set)\n",
    "\n",
    "def calculate_retrieval_recall(retrieved_indices: List[int], relevant_indices: List[int]) -> float:\n",
    "    \"\"\"Calculate recall: how many relevant chunks were retrieved\"\"\"\n",
    "    if len(relevant_indices) == 0:\n",
    "        return 1.0  # No relevant chunks to retrieve\n",
    "    \n",
    "    relevant_set = set(relevant_indices)\n",
    "    retrieved_set = set(retrieved_indices)\n",
    "    \n",
    "    correct = len(relevant_set.intersection(retrieved_set))\n",
    "    return correct / len(relevant_set)\n",
    "\n",
    "def calculate_answer_similarity(answer: str, ground_truth: str, model) -> float:\n",
    "    \"\"\"Calculate semantic similarity between generated answer and ground truth\"\"\"\n",
    "    embeddings = model.encode([answer, ground_truth], convert_to_numpy=True)\n",
    "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    return similarity\n",
    "\n",
    "def evaluate_rag(rag_system: SimpleRAG, questions: List[Dict], top_k: int = 2) -> Dict:\n",
    "    \"\"\"Evaluate RAG system on test questions\"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    similarities = []\n",
    "    \n",
    "    for q in questions:\n",
    "        result = rag_system.generate_answer(q[\"question\"], top_k)\n",
    "        retrieved_indices = [idx for idx, _, _ in result[\"retrieved_chunks\"]]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = calculate_retrieval_precision(retrieved_indices, q[\"relevant_chunks\"])\n",
    "        recall = calculate_retrieval_recall(retrieved_indices, q[\"relevant_chunks\"])\n",
    "        similarity = calculate_answer_similarity(\n",
    "            result[\"answer\"], \n",
    "            q[\"ground_truth\"], \n",
    "            rag_system.embedding_model\n",
    "        )\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    return {\n",
    "        \"avg_precision\": np.mean(precisions),\n",
    "        \"avg_recall\": np.mean(recalls),\n",
    "        \"avg_similarity\": np.mean(similarities),\n",
    "        \"precisions\": precisions,\n",
    "        \"recalls\": recalls,\n",
    "        \"similarities\": similarities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline evaluation (with all chunks)\n",
    "baseline_metrics = evaluate_rag(rag, test_questions, top_k=2)\n",
    "\n",
    "print(\"Baseline Performance (all chunks):\")\n",
    "print(f\"  Average Precision: {baseline_metrics['avg_precision']:.3f}\")\n",
    "print(f\"  Average Recall: {baseline_metrics['avg_recall']:.3f}\")\n",
    "print(f\"  Average Answer Similarity: {baseline_metrics['avg_similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Leave-One-Out (LOO) Evaluation\n",
    "\n",
    "Đánh giá giá trị của từng chunk bằng cách:\n",
    "1. Loại bỏ chunk `i` khỏi knowledge base\n",
    "2. Đánh giá lại hiệu suất RAG\n",
    "3. Tính LOO score = Performance drop khi loại bỏ chunk `i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_one_out_evaluation(chunks: List[str], questions: List[Dict], embedding_model, top_k: int = 2) -> Dict:\n",
    "    \"\"\"Perform Leave-One-Out evaluation\"\"\"\n",
    "    \n",
    "    # Baseline with all chunks\n",
    "    print(\"Calculating baseline...\")\n",
    "    baseline_rag = SimpleRAG(embedding_model, chunks)\n",
    "    baseline_metrics = evaluate_rag(baseline_rag, questions, top_k)\n",
    "    baseline_score = baseline_metrics['avg_similarity']  # Use similarity as main metric\n",
    "    \n",
    "    # LOO evaluation\n",
    "    loo_scores = []\n",
    "    loo_details = []\n",
    "    \n",
    "    print(f\"\\nRunning LOO for {len(chunks)} chunks...\")\n",
    "    for i in range(len(chunks)):\n",
    "        # Remove chunk i\n",
    "        chunks_without_i = chunks[:i] + chunks[i+1:]\n",
    "        \n",
    "        # Build RAG without chunk i\n",
    "        rag_without_i = SimpleRAG(embedding_model, chunks_without_i)\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics_without_i = evaluate_rag(rag_without_i, questions, top_k)\n",
    "        score_without_i = metrics_without_i['avg_similarity']\n",
    "        \n",
    "        # LOO score = performance drop\n",
    "        loo_score = baseline_score - score_without_i\n",
    "        loo_scores.append(loo_score)\n",
    "        \n",
    "        loo_details.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"chunk_text\": chunks[i][:100] + \"...\",\n",
    "            \"loo_score\": loo_score,\n",
    "            \"baseline_similarity\": baseline_score,\n",
    "            \"without_chunk_similarity\": score_without_i,\n",
    "            \"performance_drop\": loo_score\n",
    "        })\n",
    "        \n",
    "        print(f\"  Chunk {i}: LOO score = {loo_score:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"baseline_score\": baseline_score,\n",
    "        \"loo_scores\": loo_scores,\n",
    "        \"loo_details\": loo_details,\n",
    "        \"baseline_metrics\": baseline_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LOO evaluation\n",
    "loo_results = leave_one_out_evaluation(\n",
    "    knowledge_chunks, \n",
    "    test_questions, \n",
    "    embedding_model, \n",
    "    top_k=2\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOO Evaluation Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(loo_results['loo_details'])\n",
    "results_df = results_df.sort_values('loo_score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Most Valuable Chunks:\")\n",
    "print(results_df[['chunk_id', 'loo_score', 'chunk_text']].head())\n",
    "\n",
    "print(\"\\nBottom 5 Least Valuable Chunks:\")\n",
    "print(results_df[['chunk_id', 'loo_score', 'chunk_text']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Bar chart of LOO scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar chart\n",
    "ax1 = axes[0]\n",
    "colors = ['red' if score > 0.01 else 'lightblue' for score in results_df['loo_score']]\n",
    "ax1.barh(results_df['chunk_id'].astype(str), results_df['loo_score'], color=colors)\n",
    "ax1.set_xlabel('LOO Score (Performance Drop)', fontsize=12)\n",
    "ax1.set_ylabel('Chunk ID', fontsize=12)\n",
    "ax1.set_title('Chunk Importance (LOO Scores)', fontsize=14, fontweight='bold')\n",
    "ax1.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Heatmap of metrics\n",
    "ax2 = axes[1]\n",
    "heatmap_data = results_df[['chunk_id', 'baseline_similarity', 'without_chunk_similarity', 'performance_drop']].set_index('chunk_id')\n",
    "sns.heatmap(heatmap_data.T, annot=True, fmt='.3f', cmap='RdYlGn_r', ax=ax2, cbar_kws={'label': 'Score'})\n",
    "ax2.set_title('Performance Metrics Heatmap', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Chunk ID', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Distribution of LOO scores\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.hist(results_df['loo_score'], bins=10, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=results_df['loo_score'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {results_df['loo_score'].mean():.4f}\")\n",
    "ax.set_xlabel('LOO Score', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of Chunk Importance Scores', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpretation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SUMMARY & INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. Baseline Performance:\")\n",
    "print(f\"   - Average Similarity: {loo_results['baseline_score']:.3f}\")\n",
    "print(f\"   - Precision: {loo_results['baseline_metrics']['avg_precision']:.3f}\")\n",
    "print(f\"   - Recall: {loo_results['baseline_metrics']['avg_recall']:.3f}\")\n",
    "\n",
    "print(f\"\\n2. LOO Analysis:\")\n",
    "print(f\"   - Mean LOO Score: {np.mean(loo_results['loo_scores']):.4f}\")\n",
    "print(f\"   - Std LOO Score: {np.std(loo_results['loo_scores']):.4f}\")\n",
    "print(f\"   - Max LOO Score: {np.max(loo_results['loo_scores']):.4f} (Chunk {np.argmax(loo_results['loo_scores'])})\")\n",
    "print(f\"   - Min LOO Score: {np.min(loo_results['loo_scores']):.4f} (Chunk {np.argmin(loo_results['loo_scores'])})\")\n",
    "\n",
    "print(f\"\\n3. Most Critical Chunks:\")\n",
    "top_3 = results_df.head(3)\n",
    "for idx, row in top_3.iterrows():\n",
    "    print(f\"   - Chunk {row['chunk_id']} (LOO: {row['loo_score']:.4f}):\")\n",
    "    print(f\"     {row['chunk_text']}\")\n",
    "\n",
    "print(f\"\\n4. Least Critical Chunks:\")\n",
    "bottom_3 = results_df.tail(3)\n",
    "for idx, row in bottom_3.iterrows():\n",
    "    print(f\"   - Chunk {row['chunk_id']} (LOO: {row['loo_score']:.4f}):\")\n",
    "    print(f\"     {row['chunk_text']}\")\n",
    "\n",
    "print(f\"\\n5. Key Insights:\")\n",
    "high_value_chunks = results_df[results_df['loo_score'] > results_df['loo_score'].mean()]\n",
    "print(f\"   - {len(high_value_chunks)} chunks are above-average in importance\")\n",
    "print(f\"   - Removing critical chunks causes up to {np.max(loo_results['loo_scores']):.2%} performance drop\")\n",
    "print(f\"   - Some chunks have minimal impact (LOO score ≈ 0), indicating redundancy\")\n",
    "\n",
    "print(f\"\\n6. Recommendations:\")\n",
    "print(f\"   ✓ Focus on maintaining high-quality chunks with LOO > {results_df['loo_score'].mean():.4f}\")\n",
    "print(f\"   ✓ Consider removing or improving low-value chunks (LOO ≈ 0)\")\n",
    "print(f\"   ✓ Chunks directly answering test questions have highest LOO scores\")\n",
    "print(f\"   ✓ Scale this approach to larger datasets with Monte Carlo sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "### Scaling to Production:\n",
    "1. **Larger datasets**: Use Monte Carlo LOO for k > 10 chunks\n",
    "2. **Better metrics**: Integrate Ragas (faithfulness, answer relevancy)\n",
    "3. **Advanced retrieval**: Use vector databases (Pinecone, Weaviate)\n",
    "4. **LLM generation**: Replace concatenation with GPT/Claude\n",
    "\n",
    "### Integration with existing codebase:\n",
    "```python\n",
    "# Connect to src/dv/interfaces.py\n",
    "from src.dv.interfaces import Valuator\n",
    "\n",
    "class LOOValuator(Valuator):\n",
    "    def value(self, chunks: List[str]) -> np.ndarray:\n",
    "        # Implement LOO logic here\n",
    "        pass\n",
    "```\n",
    "\n",
    "### Experiment variations:\n",
    "- Try different embedding models (OpenAI, Cohere)\n",
    "- Vary top_k retrieval parameter\n",
    "- Test on different domains (medical, legal, etc.)\n",
    "- Compare LOO with Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV for further analysis\n",
    "results_df.to_csv('loo_results.csv', index=False)\n",
    "print(\"Results exported to loo_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
